{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from math import log10, floor\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from relational_rnn_models import RelationalMemoryGenerator\n",
    "from discriminator import RelGANDiscriminator\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Try setting the device to a GPU\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_datetime(string):\n",
    "    return datetime.datetime.strptime(string, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "icd_codes = pd.read_csv('D_ICD_DIAGNOSES.csv.gz', compression='gzip', index_col='ROW_ID')\n",
    "\n",
    "date_time_events = pd.read_csv('DATETIMEEVENTS.csv.gz', compression='gzip', index_col='ROW_ID')\n",
    "\n",
    "diagnoses = pd.read_csv('DIAGNOSES_ICD.csv.gz', compression='gzip', index_col='ROW_ID')\n",
    "\n",
    "date_time_events['CHARTTIME'] = date_time_events['CHARTTIME'].map(str_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29185\n",
      "2099-10-19 12:48:00\n",
      "2209-08-07 16:52:00\n"
     ]
    }
   ],
   "source": [
    "subjects = pd.unique(date_time_events['SUBJECT_ID'])\n",
    "print(len(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29185,)\n",
      "6080\n"
     ]
    }
   ],
   "source": [
    "li = np.zeros((len(subjects),), dtype=bool)\n",
    "print(li.shape)\n",
    "\n",
    "for i, s in enumerate(subjects):\n",
    "    li1 = diagnoses['SUBJECT_ID'] == s \n",
    "\n",
    "    subject_diagnoses = diagnoses.loc[li1]\n",
    "    \n",
    "    if len(subject_diagnoses['HADM_ID'].unique()) > 1:\n",
    "        li[i] = True\n",
    "        \n",
    "print(np.sum(li))\n",
    "subjects = subjects[li]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2099-10-19 12:48:00\n",
      "2209-08-07 16:52:00\n"
     ]
    }
   ],
   "source": [
    "t0 = np.min(date_time_events['CHARTTIME'])\n",
    "t1 = np.max(date_time_events['CHARTTIME'])\n",
    "print(t0)\n",
    "print(t1)\n",
    "\n",
    "# TODO: create a sequence for a single patient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE\n",
      "ROW_ID                                        \n",
      "85503         7657   121183      1.0     V3000\n",
      "85504         7657   121183      2.0     76515\n",
      "85505         7657   121183      3.0       769\n",
      "85506         7657   121183      4.0      7707\n",
      "85507         7657   121183      5.0      7742\n",
      "85508         7657   121183      6.0      7793\n",
      "85509         7657   121183      7.0      1123\n",
      "85510         7657   121183      8.0      6910\n",
      "85511         7657   121183      9.0     76525\n",
      "        SUBJECT_ID  HADM_ID  ICUSTAY_ID  ITEMID       CHARTTIME  \\\n",
      "ROW_ID                                                            \n",
      "703           7657   121183      297945    3411 0 days 00:00:00   \n",
      "704           7657   121183      297945    3411 0 days 01:00:00   \n",
      "705           7657   121183      297945    3411 0 days 02:30:00   \n",
      "706           7657   121183      297945    3411 0 days 04:30:00   \n",
      "707           7657   121183      297945    3411 0 days 06:30:00   \n",
      "708           7657   121183      297945    3411 0 days 07:30:00   \n",
      "709           7657   121183      297945    3411 0 days 08:30:00   \n",
      "710           7657   121183      297945    3411 0 days 09:30:00   \n",
      "711           7657   121183      297945    3411 0 days 10:30:00   \n",
      "712           7657   121183      297945    3411 0 days 12:30:00   \n",
      "713           7657   121183      297945    3411 0 days 14:30:00   \n",
      "714           7657   121183      297945    3411 0 days 16:30:00   \n",
      "715           7657   121183      297945    3411 0 days 18:30:00   \n",
      "716           7657   121183      297945    3411 0 days 21:00:00   \n",
      "717           7657   121183      297945    3411 0 days 23:30:00   \n",
      "718           7657   121183      297945    3411 1 days 00:30:00   \n",
      "719           7657   121183      297945    3411 1 days 03:00:00   \n",
      "720           7657   121183      297945    3411 1 days 04:30:00   \n",
      "721           7657   121183      297945    3411 1 days 06:30:00   \n",
      "722           7657   121183      297945    3411 1 days 23:30:00   \n",
      "\n",
      "                  STORETIME   CGID VALUE VALUEUOM  WARNING  ERROR  \\\n",
      "ROW_ID                                                              \n",
      "703     2172-03-14 01:07:00  14815   NaN     Date      NaN    NaN   \n",
      "704     2172-03-14 01:32:00  14815   NaN     Date      NaN    NaN   \n",
      "705     2172-03-14 03:14:00  14815   NaN     Date      NaN    NaN   \n",
      "706     2172-03-14 05:19:00  14815   NaN     Date      NaN    NaN   \n",
      "707     2172-03-14 07:02:00  16446   NaN     Date      NaN    NaN   \n",
      "708     2172-03-14 08:37:00  14957   NaN     Date      NaN    NaN   \n",
      "709     2172-03-14 09:10:00  16446   NaN     Date      NaN    NaN   \n",
      "710     2172-03-14 10:11:00  16446   NaN     Date      NaN    NaN   \n",
      "711     2172-03-14 11:52:00  16446   NaN     Date      NaN    NaN   \n",
      "712     2172-03-14 12:36:00  16446   NaN     Date      NaN    NaN   \n",
      "713     2172-03-14 15:10:00  14957   NaN     Date      NaN    NaN   \n",
      "714     2172-03-14 17:01:00  16446   NaN     Date      NaN    NaN   \n",
      "715     2172-03-14 19:29:00  14815   NaN     Date      NaN    NaN   \n",
      "716     2172-03-14 21:43:00  14815   NaN     Date      NaN    NaN   \n",
      "717     2172-03-15 00:27:00  14815   NaN     Date      NaN    NaN   \n",
      "718     2172-03-15 01:44:00  14815   NaN     Date      NaN    NaN   \n",
      "719     2172-03-15 03:48:00  14815   NaN     Date      NaN    NaN   \n",
      "720     2172-03-15 04:58:00  14815   NaN     Date      NaN    NaN   \n",
      "721     2172-03-15 07:17:00  19412   NaN     Date      NaN    NaN   \n",
      "722     2172-03-16 00:00:00  21570   NaN     Date      NaN    NaN   \n",
      "\n",
      "        RESULTSTATUS   STOPPED  \n",
      "ROW_ID                          \n",
      "703              NaN  NotStopd  \n",
      "704              NaN  NotStopd  \n",
      "705              NaN  NotStopd  \n",
      "706              NaN  NotStopd  \n",
      "707              NaN  NotStopd  \n",
      "708              NaN  NotStopd  \n",
      "709              NaN  NotStopd  \n",
      "710              NaN  NotStopd  \n",
      "711              NaN  NotStopd  \n",
      "712              NaN  NotStopd  \n",
      "713              NaN  NotStopd  \n",
      "714              NaN  NotStopd  \n",
      "715              NaN  NotStopd  \n",
      "716              NaN  NotStopd  \n",
      "717              NaN  NotStopd  \n",
      "718              NaN  NotStopd  \n",
      "719              NaN  NotStopd  \n",
      "720              NaN  NotStopd  \n",
      "721              NaN  NotStopd  \n",
      "722              NaN     D/C'd  \n"
     ]
    }
   ],
   "source": [
    "test_subject = date_time_events['SUBJECT_ID'].iloc[0]\n",
    "\n",
    "li = diagnoses['SUBJECT_ID'] == test_subject \n",
    "\n",
    "subject_diagnoses = diagnoses.loc[li]\n",
    "\n",
    "test_hadm = subject_diagnoses['HADM_ID'].iloc[0]\n",
    "li2 = subject_diagnoses['HADM_ID'] == test_hadm\n",
    "\n",
    "subject_diagnoses = subject_diagnoses.loc[li2]\n",
    "print(subject_diagnoses)\n",
    "\n",
    "li = date_time_events['SUBJECT_ID'] == test_subject\n",
    "\n",
    "events = date_time_events.loc[li].copy()\n",
    "events = events.sort_values('CHARTTIME')\n",
    "events['CHARTTIME'] = events['CHARTTIME'] - events['CHARTTIME'].iloc[0]\n",
    "print(events)\n",
    "\n",
    "#print(diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6439\n",
      "4485937\n",
      "14567\n",
      "14567\n",
      "4485937\n"
     ]
    }
   ],
   "source": [
    "#print(date_time_events[:5])\n",
    "\n",
    "test_subject = subjects[0]\n",
    "print(test_subject)\n",
    "li = date_time_events['SUBJECT_ID'] == test_subject\n",
    "\n",
    "#print(date_time_events.loc[li])\n",
    "print(max(date_time_events['ROW_ID']))\n",
    "print(max(icd_codes['ROW_ID']))\n",
    "print(len(icd_codes['ROW_ID'].unique()))\n",
    "print(len(date_time_events['ROW_ID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89527\n",
      "['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this']\n",
      "1143\n",
      "nobility\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "\n",
    "with open('aclImdb/imdb.vocab', 'r') as file:\n",
    "    for line in file:\n",
    "        vocab.append(line[:-1])\n",
    "        \n",
    "        \n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "print(vocab[:10])\n",
    "\n",
    "word_to_index = dict([(w, i) for i, w in enumerate(vocab)])\n",
    "index_to_word = dict([(i, w) for i, w in enumerate(vocab)])\n",
    "\n",
    "print(word_to_index['awesome'])\n",
    "print(index_to_word[12345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('aclImdb/train/pos/*.txt')[:50]\n",
    "print(len(files))\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for filename in files:\n",
    "    with open(filename, 'r') as file:\n",
    "        lengths.append(len(file.readline().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(228.9180, device='cuda:0')\n",
      "34\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(torch.tensor(lengths).type(Tensor)))\n",
    "print(min(lengths))\n",
    "print(max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5769, 0.1538, 0.1154, 0.0385, 0.0769, 0.0385], device='cuda:0')\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Define variables\n",
    "vocab_size = 6\n",
    "var_ids = list(range(vocab_size))\n",
    "var_names = ['var' + str(i) for i in var_ids]\n",
    "var_weights = torch.tensor([15, 4, 3, 1, 2, 1]).type(Tensor) \n",
    "var_weights = var_weights / torch.sum(var_weights)# variable distribution of mock data\n",
    "sequence_length = 10\n",
    "n_individuals = 2000\n",
    "\n",
    "#noise_length = 2\n",
    "print(var_weights)\n",
    "print(torch.sum(var_weights.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function(s)\n",
    "\n",
    "# round a number to n significant digits\n",
    "def round_to_n(x, n = 2):\n",
    "    return round(x, -int(floor(log10(abs(x)))) + (n - 1)) if x != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 1.9 seconds\n",
      "['var0', 'var0', 'var2', 'var0', 'var0', 'var0', 'var4', 'var2', 'var2', 'var1']\n",
      "['var0', 'var0', 'var0', 'var4', 'var0', 'var0', 'var0', 'var0', 'var0', 'var0']\n",
      "['var4', 'var1', 'var4', 'var0', 'var0', 'var2', 'var5', 'var4', 'var3', 'var0']\n",
      "['var0', 'var0', 'var0', 'var0', 'var0', 'var0', 'var1', 'var0', 'var0', 'var0']\n",
      "['var0', 'var0', 'var1', 'var4', 'var4', 'var3', 'var0', 'var0', 'var4', 'var0']\n",
      "['var0', 'var4', 'var2', 'var0', 'var0', 'var0', 'var1', 'var4', 'var0', 'var5']\n",
      "['var0', 'var3', 'var0', 'var0', 'var2', 'var0', 'var4', 'var2', 'var0', 'var5']\n",
      "['var1', 'var0', 'var0', 'var0', 'var0', 'var1', 'var2', 'var0', 'var0', 'var2']\n",
      "['var4', 'var2', 'var0', 'var0', 'var0', 'var4', 'var0', 'var1', 'var0', 'var0']\n",
      "['var1', 'var0', 'var1', 'var0', 'var1', 'var2', 'var0', 'var0', 'var4', 'var0']\n"
     ]
    }
   ],
   "source": [
    "# Generate mock data\n",
    "\n",
    "events = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "alternative_weights = torch.tensor([7, 2, 1, 2, 1, 4]).type(torch.FloatTensor) \n",
    "alternative_weights = alternative_weights / torch.sum(alternative_weights)\n",
    "\n",
    "for indv in range(n_individuals):\n",
    "    tmp = []\n",
    "    for t in range(sequence_length):\n",
    "        if t > 0 and tmp[t - 1] == 'var2':\n",
    "            var = np.random.choice(var_names, p=alternative_weights.cpu())\n",
    "        else:\n",
    "            var = np.random.choice(var_names, p=var_weights.cpu())\n",
    "        tmp.append(var)\n",
    "    events.append(tmp)\n",
    "        \n",
    "print('time taken:', round_to_n(time.time() - start_time), 'seconds')\n",
    "\n",
    "for i in range(10):\n",
    "    print(events[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'var0': 0, 'var1': 1, 'var2': 2, 'var3': 3, 'var4': 4, 'var5': 5}\n",
      "tensor([[0, 0, 2, 0, 0, 0, 4, 2, 2, 1],\n",
      "        [0, 0, 0, 4, 0, 0, 0, 0, 0, 0],\n",
      "        [4, 1, 4, 0, 0, 2, 5, 4, 3, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 4, 4, 3, 0, 0, 4, 0],\n",
      "        [0, 4, 2, 0, 0, 0, 1, 4, 0, 5],\n",
      "        [0, 3, 0, 0, 2, 0, 4, 2, 0, 5],\n",
      "        [1, 0, 0, 0, 0, 1, 2, 0, 0, 2],\n",
      "        [4, 2, 0, 0, 0, 4, 0, 1, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 2, 0, 0, 4, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vars_to_indices = dict([(v, i) for i, v in enumerate(var_names)])\n",
    "print(vars_to_indices)\n",
    "data = torch.tensor([[vars_to_indices[e] for e in event] for event in events])\n",
    "if cuda:\n",
    "    data = data.cuda()\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]], device='cuda:0')\n",
      "tensor([[[1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0.]]], device='cuda:0', grad_fn=<CatBackward>)\n",
      "tensor([[0, 3, 1, 1, 4, 5, 0, 3, 2, 4]], device='cuda:0')\n",
      "tensor([[[ 1.4425,  0.7888, -1.0418, -0.2153],\n",
      "         [ 0.1557,  0.9567, -0.7027, -1.5321],\n",
      "         [ 0.5563,  0.5732,  0.4609, -1.2281],\n",
      "         [ 0.7421,  0.5202, -0.6637,  1.0721]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test generator output\n",
    "\n",
    "mem_slots = 4\n",
    "head_size = 2\n",
    "embed_size = 2\n",
    "temperature = 1\n",
    "num_heads = 2\n",
    "\n",
    "G = RelationalMemoryGenerator(mem_slots, head_size, embed_size, vocab_size, temperature, num_heads)\n",
    "\n",
    "start_token = torch.tensor([[0]])\n",
    "memory = G.initial_state(batch_size = 1)\n",
    "\n",
    "if cuda:\n",
    "    G.cuda()\n",
    "    start_token = start_token.cuda()\n",
    "    memory = memory.cuda()\n",
    "\n",
    "print(memory)\n",
    "logits, tokens, _, memory = G(start_token, memory, sequence_length, None)\n",
    "print(logits)\n",
    "print(tokens)\n",
    "print(memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5115, device='cuda:0') tensor(1.1068, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Define generator evaluation functions\n",
    "\n",
    "def eval_generator(G, data, vocab_size):\n",
    "    memory = G.initial_state(batch_size = data.shape[0])\n",
    "    if cuda:\n",
    "        memory = memory.cuda()\n",
    "    _, data_fake, _, _ = G(data[:, :1], memory, data.shape[1])\n",
    "    word_means = torch.stack([torch.mean((data == i).type(Tensor), dim = 0) for i in range(vocab_size)])\n",
    "    word_means_fake = torch.stack([torch.mean((data_fake == i).type(Tensor), dim = 0) for i in range(vocab_size)])\n",
    "    \n",
    "    scores = torch.sum(torch.abs(word_means - word_means_fake), dim = 1)\n",
    "    \n",
    "    return scores # for each word; the lower the better\n",
    "\n",
    "def count_special_cases(data, vocab_size):\n",
    "    counts1 = torch.zeros(vocab_size)\n",
    "    counts2 = torch.zeros(vocab_size)\n",
    "    \n",
    "    if cuda:\n",
    "        counts1 = counts1.cuda()\n",
    "        counts2 = counts2.cuda()\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for t in range(data.shape[1] - 1):\n",
    "            if data[i, t] == 2:\n",
    "                counts1[data[i, t + 1]] += 1\n",
    "            else:\n",
    "                counts2[data[i, t + 1]] += 1\n",
    "                \n",
    "    return counts1, counts2\n",
    "\n",
    "def chi_sqrd_dist(counts1, counts2):\n",
    "    counts1 = counts1.view(1, -1)\n",
    "    counts2 = counts2.view(1, -1)\n",
    "    table = torch.cat([counts1, counts2], dim = 0)\n",
    "    col_sums = torch.sum(table, dim = 0)\n",
    "    row_sums = torch.sum(table, dim = 1)\n",
    "    n = torch.sum(col_sums)\n",
    "    \n",
    "    table_freq = table / n\n",
    "    col_freqs = col_sums / n\n",
    "    row_freqs = row_sums / n\n",
    "    \n",
    "    diffs = table_freq[0, :] / row_freqs[0] - table_freq[1, :] / row_freqs[1]\n",
    "    diffs_sqrd = diffs ** 2\n",
    "    diffs_sqrd_norm = diffs_sqrd / col_freqs\n",
    "    \n",
    "    chi_sqrd_distance = torch.sum(diffs_sqrd_norm)\n",
    "    \n",
    "    return chi_sqrd_distance\n",
    "    \n",
    "\n",
    "def test_special_case(G, data, vocab_size, return_freq = True):\n",
    "    counts_real1, counts_real2 = count_special_cases(data, vocab_size)\n",
    "    freq_real1 = counts_real1 / torch.sum(counts_real1)\n",
    "    freq_real2 = counts_real2 / torch.sum(counts_real2)\n",
    "    \n",
    "    memory = G.initial_state(batch_size = data.shape[0])\n",
    "\n",
    "    if cuda:\n",
    "        memory = memory.cuda()\n",
    "    \n",
    "    _, data_fake, _, _ = G(data[:, :1], memory, data.shape[1])\n",
    "    \n",
    "    counts_fake1, counts_fake2 = count_special_cases(data_fake, vocab_size)\n",
    "    freq_fake1 = counts_fake1 / torch.sum(counts_fake1)\n",
    "    freq_fake2 = counts_fake2 / torch.sum(counts_fake2)\n",
    "    \n",
    "    if return_freq:\n",
    "        return freq_fake1, freq_fake2\n",
    "    else:\n",
    "        score1 = chi_sqrd_dist(counts_real1, counts_fake1)\n",
    "        score2 = chi_sqrd_dist(counts_real2, counts_fake2)\n",
    "        return score1, score2\n",
    "\n",
    "score1, score2 = test_special_case(G, data, vocab_size, False)\n",
    "print(score1, score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0.]]], device='cuda:0', grad_fn=<CatBackward>)\n",
      "tensor([[0.4686, 0.4712]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.4699], device='cuda:0', grad_fn=<MeanBackward2>)\n"
     ]
    }
   ],
   "source": [
    "# Test Discriminator output\n",
    "\n",
    "n_embeddings = 2\n",
    "embed_size = 2\n",
    "out_channels = 5 \n",
    "filter_sizes = [2, 3] # values can be at most the sequence_length\n",
    "\n",
    "D = RelGANDiscriminator(n_embeddings, vocab_size, embed_size, sequence_length, out_channels, filter_sizes)\n",
    "\n",
    "if cuda:\n",
    "    D.cuda()\n",
    "\n",
    "inp = logits\n",
    "print(inp)\n",
    "print(D(inp, False))\n",
    "print(D(inp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator pre-train function\n",
    "\n",
    "def pretrain_generator(G, train_data, vocab_size, n_epochs, lr, print_step = 10):\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "    \n",
    "    train_data_one_hot = F.one_hot(train_data, vocab_size).type(Tensor)\n",
    "    start_token = train_data[:, :1]\n",
    "    sequence_length = train_data.shape[1]\n",
    "    \n",
    "    if cuda:\n",
    "        G.cuda()\n",
    "        loss_function.cuda()\n",
    "        start_token = start_token.cuda()\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        memory = G.initial_state(batch_size = train_data.shape[0])\n",
    "        \n",
    "        if cuda:\n",
    "            memory = memory.cuda()\n",
    "        \n",
    "        logits, _, _, _ = G(start_token, memory, sequence_length)\n",
    "        \n",
    "        loss = loss_function(logits, train_data_one_hot)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if e % print_step == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [G loss: %f]\"\n",
    "                % (e, n_epochs, loss.item())\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.4210, 1.0360, 0.1360, 1.4230, 0.2630, 1.2030], device='cuda:0') tensor([ 5.9297,  6.7340,  1.1787, 36.9980,  3.4190, 31.2780], device='cuda:0')\n",
      "[Epoch 0/10] [G loss: 6.857559]\n",
      "[Epoch 2/10] [G loss: 6.731378]\n",
      "[Epoch 4/10] [G loss: 6.696838]\n",
      "[Epoch 6/10] [G loss: 6.599209]\n",
      "[Epoch 8/10] [G loss: 6.561907]\n",
      "tensor([2.5485, 0.4595, 0.1650, 1.0030, 0.1460, 1.0930], device='cuda:0') tensor([ 4.4174,  2.9867,  1.4300, 26.0780,  1.8980, 28.4180], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mem_slots = 4\n",
    "head_size = 2\n",
    "embed_size = 2\n",
    "temperature = 1\n",
    "num_heads = 2\n",
    "\n",
    "G = RelationalMemoryGenerator(mem_slots, head_size, embed_size, vocab_size, temperature, num_heads)\n",
    "\n",
    "if cuda:\n",
    "    G.cuda()\n",
    "\n",
    "# TODO: change to the better evaluation functions\n",
    "scores = eval_generator(G, data, vocab_size)\n",
    "print(scores, scores / var_weights)\n",
    "pretrain_generator(G, data, vocab_size, 10, 0.001, 2)\n",
    "scores = eval_generator(G, data, vocab_size)\n",
    "print(scores, scores / var_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "\n",
    "def train(G, D, train_data, vocab_size, n_epochs, lr, temperature, print_step = 10, score_fn = test_special_case):    \n",
    "    scores = []\n",
    "    \n",
    "    score1, score2 = test_special_case(G, data, vocab_size, False)\n",
    "    print('scores before training:', score1, score2)\n",
    "    scores.append(torch.tensor([score1, score2]))\n",
    "    \n",
    "    print('pretraining generator...')\n",
    "    pretrain_generator(G, train_data, vocab_size, max(n_epochs // 10, 1), lr, max(n_epochs // 10 - 1, 1))\n",
    "    print('pretraining complete')\n",
    "    \n",
    "    score1, score2 = score_fn(G, train_data, vocab_size, False)\n",
    "    print(\"[Scores:\", score1, score2, \"]\")\n",
    "    scores.append(torch.tensor([score1, score2]))\n",
    "    \n",
    "    adversarial_loss = torch.nn.BCELoss()\n",
    "    \n",
    "    optimizer_G = torch.optim.Adam(G.parameters(), lr=lr)\n",
    "    optimizer_D = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "    \n",
    "    train_data_one_hot = F.one_hot(train_data, vocab_size).type(Tensor)\n",
    "\n",
    "    start_token = train_data[:, :1]\n",
    "    sequence_length = train_data.shape[1]\n",
    "    \n",
    "    if cuda:\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "        adversarial_loss.cuda()\n",
    "        start_token = start_token.cuda()\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(train_data.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(train_data.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        memory = G.initial_state(batch_size = train_data.shape[0])\n",
    "        if cuda:\n",
    "            memory = memory.cuda()\n",
    "        temp = temperature ** ((e + 1) / n_epochs)\n",
    "        fake_one_hot, _, _, _ = G(start_token, memory, sequence_length, temp)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(D(fake_one_hot), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(D(train_data_one_hot), valid)\n",
    "        fake_loss = adversarial_loss(D(fake_one_hot.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        if e % print_step == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (e, n_epochs, d_loss.item(), g_loss.item())\n",
    "            )\n",
    "            #print(\"[Frequencies:\", score_fn(G, train_data, vocab_size), \"]\")\n",
    "            score1, score2 = score_fn(G, train_data, vocab_size, False)\n",
    "            print(\"[Scores:\", score1, score2, \"]\")\n",
    "            scores.append(torch.tensor([score1, score2]))\n",
    "            \n",
    "    score1, score2 = test_special_case(G, data, vocab_size, False)\n",
    "    print('scores after training:', score1, score2)\n",
    "    scores.append(torch.tensor([score1, score2]))\n",
    "            \n",
    "    #return scores\n",
    "    return torch.stack(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributions before training: tensor([0.1218, 0.1561, 0.1683, 0.2122, 0.1901, 0.1513], device='cuda:0') tensor([0.0964, 0.1612, 0.1840, 0.1982, 0.1958, 0.1644], device='cuda:0')\n",
      "benchmark distributions: tensor([0.4118, 0.1176, 0.0588, 0.1176, 0.0588, 0.2353]) tensor([0.5769, 0.1538, 0.1154, 0.0385, 0.0769, 0.0385], device='cuda:0')\n",
      "scores before training: tensor(0.7574, device='cuda:0') tensor(1.1974, device='cuda:0')\n",
      "pretraining generator...\n",
      "[Epoch 0/200] [G loss: 7.192816]\n",
      "[Epoch 199/200] [G loss: 3.745845]\n",
      "pretraining complete\n",
      "[Scores: tensor(0.7131, device='cuda:0') tensor(1.0300, device='cuda:0') ]\n",
      "[Epoch 0/2000] [D loss: 0.701670] [G loss: 0.743551]\n",
      "[Scores: tensor(0.8411, device='cuda:0') tensor(1.0390, device='cuda:0') ]\n",
      "[Epoch 100/2000] [D loss: 0.665388] [G loss: 0.690467]\n",
      "[Scores: tensor(0.1884, device='cuda:0') tensor(0.8270, device='cuda:0') ]\n",
      "[Epoch 200/2000] [D loss: 0.697724] [G loss: 0.667679]\n",
      "[Scores: tensor(0.8864, device='cuda:0') tensor(3.1971, device='cuda:0') ]\n",
      "[Epoch 300/2000] [D loss: 0.446337] [G loss: 0.987409]\n",
      "[Scores: tensor(2.4636, device='cuda:0') tensor(2.6201, device='cuda:0') ]\n",
      "[Epoch 400/2000] [D loss: 0.491020] [G loss: 0.962070]\n",
      "[Scores: tensor(1.1412, device='cuda:0') tensor(2.0967, device='cuda:0') ]\n",
      "[Epoch 500/2000] [D loss: 0.597900] [G loss: 0.681091]\n",
      "[Scores: tensor(1.3881, device='cuda:0') tensor(0.5045, device='cuda:0') ]\n",
      "[Epoch 600/2000] [D loss: 0.724374] [G loss: 0.675011]\n",
      "[Scores: tensor(0.5374, device='cuda:0') tensor(0.1547, device='cuda:0') ]\n",
      "[Epoch 700/2000] [D loss: 0.720232] [G loss: 0.667823]\n",
      "[Scores: tensor(0.4741, device='cuda:0') tensor(0.0488, device='cuda:0') ]\n",
      "[Epoch 800/2000] [D loss: 0.701393] [G loss: 0.680049]\n",
      "[Scores: tensor(0.6160, device='cuda:0') tensor(0.0048, device='cuda:0') ]\n",
      "[Epoch 900/2000] [D loss: 0.702431] [G loss: 0.685510]\n",
      "[Scores: tensor(0.2072, device='cuda:0') tensor(0.0340, device='cuda:0') ]\n",
      "[Epoch 1000/2000] [D loss: 0.686831] [G loss: 0.753136]\n",
      "[Scores: tensor(0.8933, device='cuda:0') tensor(0.1174, device='cuda:0') ]\n",
      "[Epoch 1100/2000] [D loss: 0.682615] [G loss: 0.695639]\n",
      "[Scores: tensor(0.8418, device='cuda:0') tensor(0.3010, device='cuda:0') ]\n",
      "[Epoch 1200/2000] [D loss: 0.692537] [G loss: 0.658406]\n",
      "[Scores: tensor(0.3926, device='cuda:0') tensor(0.1471, device='cuda:0') ]\n",
      "[Epoch 1300/2000] [D loss: 0.801063] [G loss: 0.823025]\n",
      "[Scores: tensor(0.8134, device='cuda:0') tensor(0.8735, device='cuda:0') ]\n",
      "[Epoch 1400/2000] [D loss: 0.702706] [G loss: 0.651211]\n",
      "[Scores: tensor(0.2961, device='cuda:0') tensor(0.0926, device='cuda:0') ]\n",
      "[Epoch 1500/2000] [D loss: 0.707467] [G loss: 0.741597]\n",
      "[Scores: tensor(0.7454, device='cuda:0') tensor(0.1319, device='cuda:0') ]\n",
      "[Epoch 1600/2000] [D loss: 0.701569] [G loss: 0.728510]\n",
      "[Scores: tensor(0.2973, device='cuda:0') tensor(0.2075, device='cuda:0') ]\n",
      "[Epoch 1700/2000] [D loss: 0.704644] [G loss: 0.713501]\n",
      "[Scores: tensor(0.4193, device='cuda:0') tensor(0.0734, device='cuda:0') ]\n",
      "[Epoch 1800/2000] [D loss: 0.690708] [G loss: 0.729829]\n",
      "[Scores: tensor(0.3754, device='cuda:0') tensor(0.0434, device='cuda:0') ]\n",
      "[Epoch 1900/2000] [D loss: 0.691695] [G loss: 0.714251]\n",
      "[Scores: tensor(0.2301, device='cuda:0') tensor(0.2091, device='cuda:0') ]\n",
      "scores after training: tensor(0.2594, device='cuda:0') tensor(0.2610, device='cuda:0')\n",
      "distributions after training: tensor([0.6246, 0.0372, 0.0372, 0.1479, 0.0675, 0.0856], device='cuda:0') tensor([0.6519, 0.0639, 0.0612, 0.0354, 0.1835, 0.0041], device='cuda:0')\n",
      "benchmark distributions: tensor([0.4118, 0.1176, 0.0588, 0.1176, 0.0588, 0.2353]) tensor([0.5769, 0.1538, 0.1154, 0.0385, 0.0769, 0.0385], device='cuda:0')\n",
      "time taken: 1700.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the GAN\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generator params\n",
    "mem_slots = 1\n",
    "head_size = 4\n",
    "embed_size = 3\n",
    "temperature = 50\n",
    "num_heads = 6\n",
    "num_blocks = 4\n",
    "\n",
    "G = RelationalMemoryGenerator(mem_slots, head_size, embed_size, vocab_size, temperature, num_heads, num_blocks)\n",
    "\n",
    "# Discriminator params\n",
    "n_embeddings = 3\n",
    "embed_size = embed_size\n",
    "out_channels = 10\n",
    "filter_sizes = [2, 3, 4] # values can be at most the sequence_length\n",
    "\n",
    "D = RelGANDiscriminator(n_embeddings, vocab_size, embed_size, sequence_length, out_channels, filter_sizes)\n",
    "\n",
    "if cuda:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "\n",
    "#scores = eval_generator(G, data, vocab_size)\n",
    "scores1, scores2 = test_special_case(G, data, vocab_size)\n",
    "print('distributions before training:', scores1, scores2)\n",
    "print('benchmark distributions:', alternative_weights, var_weights)\n",
    "\n",
    "# Train the GAN\n",
    "scores = train(G, D, data, vocab_size, 2000, 0.001, temperature, 100)\n",
    "\n",
    "#scores = eval_generator(G, data, vocab_size)\n",
    "scores1, scores2 = test_special_case(G, data, vocab_size)\n",
    "print('distributions after training:', scores1, scores2)\n",
    "print('benchmark distributions:', alternative_weights, var_weights)\n",
    "\n",
    "print('time taken:', round_to_n(time.time() - start_time), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(scores.shape[0]), scores[:, 0].numpy())\n",
    "plt.plot(range(scores.shape[0]), scores[:, 1].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
