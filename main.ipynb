{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15962041257352351903\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16591054485076167791\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4424036384668240335\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11276946637\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15242822112926518825\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from math import log10, floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "n_vars = 3\n",
    "var_ids = list(range(n_vars))\n",
    "var_names = ['var' + str(i) for i in var_ids]\n",
    "var_weights = [0.1, 0.6, 0.3] # variable distribution of mock data\n",
    "n_time_steps = 20\n",
    "n_individuals = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function(s)\n",
    "\n",
    "# round a number to n significant digits\n",
    "def round_to_n(x, n = 2):\n",
    "    return round(x, -int(floor(log10(abs(x)))) + (n - 1)) if x != 0 else 0\n",
    "\n",
    "def visualize_output(generator, z, n=2):\n",
    "    p = np.reshape(generator.predict(z), (n_time_steps, n_vars))\n",
    "    p.shape\n",
    "    for t in range(p.shape[0]):\n",
    "        tmp = []\n",
    "        for f in range(p.shape[1]):\n",
    "            tmp.append(round_to_n(p[t,f], n))\n",
    "        print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 5.1 seconds\n"
     ]
    }
   ],
   "source": [
    "# Generate mock data\n",
    "\n",
    "data = np.zeros(shape=(n_individuals, n_time_steps, n_vars))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for indv in range(n_individuals):\n",
    "    for t in range(n_time_steps):\n",
    "        var = np.random.choice(var_ids, p=var_weights)\n",
    "        data[indv][t][var] = 1\n",
    "        \n",
    "print('time taken:', round_to_n(time.time() - start_time), 'seconds')\n",
    "        \n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1009, 0.5885, 0.3106],\n",
       "       [0.1012, 0.5952, 0.3036],\n",
       "       [0.105 , 0.5953, 0.2997],\n",
       "       [0.1007, 0.6035, 0.2958],\n",
       "       [0.0968, 0.6109, 0.2923],\n",
       "       [0.1011, 0.599 , 0.2999],\n",
       "       [0.0952, 0.602 , 0.3028],\n",
       "       [0.0997, 0.6086, 0.2917],\n",
       "       [0.0982, 0.6037, 0.2981],\n",
       "       [0.1039, 0.6   , 0.2961],\n",
       "       [0.1017, 0.6059, 0.2924],\n",
       "       [0.0988, 0.5932, 0.308 ],\n",
       "       [0.0977, 0.607 , 0.2953],\n",
       "       [0.0998, 0.5988, 0.3014],\n",
       "       [0.1031, 0.5957, 0.3012],\n",
       "       [0.0994, 0.6056, 0.295 ],\n",
       "       [0.0994, 0.5891, 0.3115],\n",
       "       [0.1011, 0.602 , 0.2969],\n",
       "       [0.0985, 0.5967, 0.3048],\n",
       "       [0.1044, 0.601 , 0.2946]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indv_sum = np.sum(data, axis=0)\n",
    "indv_sum / n_individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 20, 3) (2000, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into train and test sets \n",
    "\n",
    "data_train, data_test = train_test_split(data, test_size = 0.2)\n",
    "print(data_train.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 20, 3)             0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 20, 1)             20        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 20, 1)             2         \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the discriminator\n",
    "\n",
    "inp_d = Input(shape=(n_time_steps, n_vars))\n",
    "lstm_d = LSTM(1, return_sequences=True)(inp_d)\n",
    "out_d = Dense(1, activation='sigmoid')(lstm_d)\n",
    "discriminator = Model(inp_d, out_d, name='discriminator')\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_train[0]\n",
    "discriminator.predict(np.reshape(x, (1, n_time_steps, n_vars))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 20, 5)             0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 20, 3)             108       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 20, 3)             12        \n",
      "=================================================================\n",
      "Total params: 120\n",
      "Trainable params: 120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the generator\n",
    "\n",
    "noise_length = 5\n",
    "\n",
    "inp_g = Input(shape=(n_time_steps, noise_length))\n",
    "lstm_g = LSTM(3, return_sequences=True)(inp_g)\n",
    "out_g = Dense(3, activation='softmax')(lstm_g)\n",
    "generator = Model(inp_g, out_g, name='generator')\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.normal(scale = 0.5, size = (1, n_time_steps, noise_length))\n",
    "generator.predict(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator (Model)        (None, 20, 1)             22        \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the trainable discriminator model\n",
    "discriminator.trainable = True\n",
    "\n",
    "discriminator_model = Sequential()\n",
    "discriminator_model.add(discriminator)\n",
    "discriminator_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator (Model)            (None, 20, 3)             120       \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 20, 1)             22        \n",
      "=================================================================\n",
      "Total params: 142\n",
      "Trainable params: 120\n",
      "Non-trainable params: 22\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the trainable adversarial model\n",
    "discriminator.trainable = False\n",
    "\n",
    "adversarial_model = Sequential()\n",
    "adversarial_model.add(generator)\n",
    "adversarial_model.add(discriminator)\n",
    "adversarial_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "adversarial_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a train function\n",
    "\n",
    "def train(batch_size = 50, epochs = 10, print_step = 1):\n",
    "    for epoch in range(epochs):\n",
    "        idx_true = np.random.choice(data_train.shape[0], size = batch_size, replace = False)\n",
    "        x_true = data_train[idx_true, :, :]\n",
    "        z = np.random.normal(scale = 1, size = (batch_size, n_time_steps, noise_length))\n",
    "        \n",
    "        x_fake = generator.predict(z)\n",
    "        \n",
    "        x = np.concatenate((x_true, x_fake))\n",
    "        y = np.ones([2 * batch_size, 20, 1])\n",
    "        y[batch_size:, :, :] = 0\n",
    "        discriminator.trainable = True\n",
    "        d_loss = discriminator_model.train_on_batch(x, y)\n",
    "        \n",
    "        y = np.ones([batch_size, 20, 1])\n",
    "        discriminator.trainable = False\n",
    "        z = np.random.normal(scale = 1, size = (batch_size, n_time_steps, noise_length)) \n",
    "        a_loss = adversarial_model.train_on_batch(z, y)\n",
    "        \n",
    "        if (epoch % print_step) == 0:\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (epoch, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.673320, acc: 0.673750]  [A loss: 0.738642, acc: 0.050000]\n",
      "50: [D loss: 0.672162, acc: 0.686750]  [A loss: 0.738330, acc: 0.068000]\n",
      "100: [D loss: 0.670003, acc: 0.540750]  [A loss: 0.847548, acc: 0.065000]\n",
      "150: [D loss: 0.656923, acc: 0.558250]  [A loss: 0.843167, acc: 0.132000]\n",
      "200: [D loss: 0.642969, acc: 0.619000]  [A loss: 0.851687, acc: 0.172500]\n",
      "250: [D loss: 0.626151, acc: 0.679000]  [A loss: 0.878341, acc: 0.185000]\n",
      "300: [D loss: 0.604707, acc: 0.750250]  [A loss: 0.899589, acc: 0.190000]\n",
      "350: [D loss: 0.586880, acc: 0.785750]  [A loss: 0.913796, acc: 0.199500]\n",
      "400: [D loss: 0.575091, acc: 0.800750]  [A loss: 0.930761, acc: 0.203000]\n",
      "450: [D loss: 0.561313, acc: 0.808750]  [A loss: 0.946859, acc: 0.210500]\n",
      "500: [D loss: 0.545479, acc: 0.823750]  [A loss: 0.963473, acc: 0.212000]\n",
      "550: [D loss: 0.534572, acc: 0.826750]  [A loss: 0.985672, acc: 0.214000]\n",
      "600: [D loss: 0.519236, acc: 0.839750]  [A loss: 0.998054, acc: 0.230000]\n",
      "650: [D loss: 0.509956, acc: 0.838750]  [A loss: 1.025991, acc: 0.222500]\n",
      "700: [D loss: 0.504856, acc: 0.837500]  [A loss: 1.054056, acc: 0.215000]\n",
      "750: [D loss: 0.491427, acc: 0.848500]  [A loss: 1.065602, acc: 0.230500]\n",
      "800: [D loss: 0.487084, acc: 0.847500]  [A loss: 1.088806, acc: 0.218000]\n",
      "850: [D loss: 0.479257, acc: 0.856500]  [A loss: 1.112456, acc: 0.200000]\n",
      "900: [D loss: 0.475443, acc: 0.851500]  [A loss: 1.129041, acc: 0.200000]\n",
      "950: [D loss: 0.477955, acc: 0.839750]  [A loss: 1.144832, acc: 0.200000]\n",
      "time taken: 200.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# run training function\n",
    "\n",
    "start_time = time.time()\n",
    "train(batch_size = 100, epochs = 1000, print_step = 50)\n",
    "print('time taken:', round_to_n(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04, 0.9, 0.02]\n",
      "[0.01, 1.0, 0.003]\n",
      "[0.005, 1.0, 0.002]\n",
      "[0.005, 1.0, 0.002]\n",
      "[0.004, 1.0, 0.001]\n",
      "[0.004, 1.0, 0.002]\n",
      "[0.006, 1.0, 0.001]\n",
      "[0.003, 1.0, 0.001]\n",
      "[0.004, 1.0, 0.001]\n",
      "[0.005, 1.0, 0.001]\n",
      "[0.004, 1.0, 0.001]\n",
      "[0.004, 1.0, 0.001]\n",
      "[0.004, 1.0, 0.002]\n",
      "[0.005, 1.0, 0.001]\n",
      "[0.004, 1.0, 0.001]\n",
      "[0.004, 1.0, 0.001]\n",
      "[0.003, 1.0, 0.001]\n",
      "[0.003, 1.0, 0.001]\n",
      "[0.004, 1.0, 0.002]\n",
      "[0.005, 1.0, 0.001]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.normal(scale = 0.5, size = (1, n_time_steps, noise_length))\n",
    "visualize_output(generator, z, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
